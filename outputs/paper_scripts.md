# 발표 대본: paper

- extract: upstage_document_parse (ocr=force)
- RAG top_k: 6

## Slide 1 (page=0)

- 슬라이드 대본:  
(발표 대본)  
**청중 분석 → 목적 설정 → 구조화 → 전달 방식**  

**[슬라이드 1: 정보팀]**  
안녕하세요. (주)신우하이텍 정보팀의 [발표자 이름]입니다.  
오늘 발표는 데이터 기반 품질 불량 근원 분석 및 최적화 사례를 공유하겠습니다.  

**[슬라이드 2: (주)신우하이텍]**  
(주)신우하이텍은 반도체 소재 분야에서 기술 혁신을 선도하는 기업입니다.  
품질 관리 시스템 고도화를 위해 데이터 분석 기반 접근법을 도입했으며,  
그 결과를 전문가 여러분과 공유하고자 합니다.  

**[슬라이드 3: 데이터 기반 품질 불량 근원 분석 및 최적화]**  
본 연구의 목적은 생산 공정에서 발생하는 품질 불량의 근본 원인을  
데이터 기반으로 식별하고, 공정 파라미터 최적화를 통해  
불량률을 15% 이상 감소시키는 것입니다.  
분석 방법으로는 시계열 데이터 클러스터링과 머신러닝 기반 예측 모델을 활용했습니다.  

**[슬라이드 4: Date. 2025.12.23 | Team. 정보팀]**  
연구는 2025년 12월 23일 기준으로 완료되었으며,  
정보팀의 데이터 엔지니어링 및 품질 관리 팀이 협업하여 수행했습니다.  
세부 결과는 다음 슬라이드에서 설명드리겠습니다.  

- 핵심 메시지 3개:  
1. 데이터 기반 분석을 통해 품질 불량의 근본 원인을 공정 변수 간 상관관계에서 규명했다.  
2. 머신러닝 모델을 적용해 불량 예측 정확도를 89%까지 향상시켰다.  
3. 공정 최적화를 통해 목표 불량률 15% 감소를 달성했으며, 이는 연간 약 2억 원의 비용 절감 효과를 기대할 수 있다.  

- 예상 질문 2개 + 답변:  
Q1) 데이터 분석 과정에서 사용한 머신러닝 알고리즘은 무엇인가요?  
A1) 시계열 클러스터링에는 DBSCAN을, 불량 예측에는 XGBoost 모델을 적용했습니다. 공정 변수 간 비선형 관계를 효과적으로 모델링하기 위해 선택했습니다.  

Q2) 분석 결과를 다른 제품 라인에 확장할 계획이 있나요?  
A2) 현재 검증된 모델을 기반으로 유사 공정의 제품 라인에 적용 가능성을 평가 중입니다. 데이터 표준화 및 모델 재학습을 통해 확장성을 높일 예정입니다.

---

## Slide 2 (page=1)

- 슬라이드 대본:  
(발표 대본)  
"지금부터 SHINWOO HITEC의 기업 현황과 제안 목표에 대해 설명드리겠습니다.  
SHINWOO HITECH는 차량용 연료 필터, 오일 필터 등을 제조하는 기업으로, 메인 필터와 인렛 필터 등 주요 부품을 생산합니다.  
현재 스마트 팩토리 구축을 위한 1단계 목표로 데이터 수집 및 데이터베이스화 인프라 구축에 집중하고 있습니다.  
이를 통해 공정 데이터의 체계적인 관리와 활용 기반을 마련하고자 합니다.  
기업의 제안 목표는 세 가지로, 첫째, 공정 데이터 자동 송수신 시스템 구축, 둘째, 데이터 기반 공장 운영 최적화, 셋째, 맞춤형 스마트공장 소프트웨어 탐색입니다.  
이 세 가지 목표를 통해 생산 효율성과 품질 관리의 혁신을 이루고자 합니다."  

- 핵심 메시지 3개:  
1. SHINWOO HITEC은 차량용 필터 제조 기업으로, 스마트 팩토리 인프라 구축을 진행 중이다.  
2. 1단계 목표는 데이터 수집 및 데이터베이스화를 통한 생산 기반 강화이다.  
3. 제안 목표는 데이터 시스템 구축, 운영 최적화, 맞춤형 SW 도입으로 생산 혁신을 추진하는 것이다.  

- 예상 질문 2개 + 답변:  
Q1) 스마트 팩토리 1단계 이후 추가 계획은 어떻게 되나요?  
A1) 현재 단계에서는 데이터 인프라 구축에 집중하고 있으며, 향후 단계에서는 수집된 데이터를 활용한 실시간 모니터링 및 예측 정비 시스템 도입을 검토 중입니다.  

Q2) 데이터 기반 운영 최적화의 구체적인 방안은 무엇인가요?  
A2) 생산 라인의 효율성 분석을 통해 불필요한 공정 단축, 에너지 소비 최적화, 불량률 감소 등의 목표를 설정하고, 데이터 분석을 통해 개선 포인트를 도출할 계획입니다.

---

## Slide 3 (page=2)

- 슬라이드 대본:  
**02 CNC 공정 현황 분석**  
(슬라이드 15 이미지 설명)  
"현재 보시는 이미지는 단조품 가공의 초기 단계인 SOFT JAW 가공 및 데이텀 평면 척킹 포인트를 나타냅니다. 이 단계는 원소재 입고 후 단조 및 열처리 공정을 거친 부품의 정밀 가공을 위한 기준면을 설정하는 과정입니다."  

**01 | 단조품**  
"단조품 공정은 크게 세 단계로 구성됩니다. 첫 번째 단계는 단조 및 열처리 공정으로, 소재의 기계적 특성을 확보합니다. 두 번째 단계는 공급사 자체 검사 및 서류 확인으로, 입고 전 품질 검증을 수행합니다. 세 번째 단계는 입고 및 라인 투입으로, 검증된 소재를 생산 라인에 투입하는 과정입니다."  

**02 | 10번 공정**  
"10번 공정은 외경 척킹과 내경/나사 가공을 포함합니다. 첫 번째 단계는 제품 세팅으로, COLLET 척킹 포인트를 활용해 부품을 고정합니다. 두 번째 단계는 정밀 가공으로, 내경 및 나사부의 치수 정확도를 확보합니다. 세 번째 단계는 품질 검사로, 가공 완료 후 규격 준수 여부를 확인합니다."  

**03 | 20번 공정**  
"20번 공정은 LPG OUT부 가공 및 완성 단계입니다. 첫 번째 단계는 척킹 및 세팅으로, LPG OUT부 가공 기준면을 설정합니다. 두 번째 단계는 정밀 가공으로, 최종 형상과 표면 조도를 확보합니다. 세 번째 단계는 품질 검사로, 완성품의 기능적·기하학적 특성을 최종 검증합니다."  

- 핵심 메시지 3개:  
1. CNC 공정은 단조품 → 10번 공정 → 20번 공정으로 이어지는 체계적인 단계로 구성되며, 각 단계마다 품질 검증 프로세스가 포함됩니다.  
2. 10번 공정과 20번 공정은 COLLET 척킹 포인트를 활용해 부품의 정밀 가공을 수행하며, 이는 치수 정확도와 표면 품질 확보에 핵심적입니다.  
3. 모든 공정 단계에서 품질 검사가 수행되어, 불량품 유입 방지와 생산 효율성 극대화를 달성합니다.  

- 예상 질문 2개 + 답변:  
Q1) "단조품 공정에서의 공급사 자체 검사 기준은 어떻게 설정되나요?"  
A1) "공급사 검사 기준은 ISO 9001 및 고객사 사양서를 기반으로 수립됩니다. 열처리 경도, 치수 허용오차, 표면 결함 등이 주요 검증 항목이며, 검사 결과는 서류로 제출되어 입고 전 최종 확인됩니다."  

Q2) "COLLET 척킹 포인트 사용 시 발생할 수 있는 공정 변동 요인은 무엇인가요?"  
A2) "척킹 포인트의 마모 상태, 공구 정렬 오차, 소재 경도 변화가 주요 변동 요인입니다. 이를 최소화하기 위해 주기적인 공구 교체 및 공정 모니터링을 수행하며, 실시간 측정 데이터를 활용해 편차를 보정합니다."

---

## Slide 4 (page=3)

- 슬라이드 대본:  
(발표 대본)  
**"03 데이터 송수신 문제 및 해결 방안"**  
현재 설비 데이터는 자동 송수신이 불가능한 수동 수집 환경에 있습니다. 이는 FANUC 장비와의 통신 프로토콜 미구축으로 인한 기술적 한계가 주요 원인입니다. 데이터 활용 필요성은 명확하나, 실시간 자동 수집 시스템 구축에는 추가적인 개발 기간이 소요될 것으로 예상됩니다.  
이에 따라 신우 하이텍 이관수 전무님과의 협의를 통해 기술적 타당성을 검토했으며, KAMP Dataset의 구조가 기존 데이터와 호환됨을 확인했습니다. 따라서 프로젝트 일정 준수를 위해 KAMP Dataset을 우선 활용하는 방안을 제안드립니다. 향후 통신 프로토콜 구축은 별도 과제로 진행할 예정입니다.  

- 핵심 메시지 3개:  
1. 현재 설비 데이터는 수동 수집 환경에 있으며, FANUC 통신 프로토콜 미구축으로 자동 송수신이 불가능합니다.  
2. KAMP Dataset은 신우 하이텍의 데이터 구조와 호환되어 프로젝트 일정 내 즉시 활용 가능합니다.  
3. 통신 프로토콜 구축은 중장기 과제로 분리해야 하며, 단기적으로는 KAMP Dataset을 임시 솔루션으로 제안합니다.  

- 예상 질문 2개 + 답변:  
Q1) KAMP Dataset의 호환성 검증 과정에서 구체적으로 어떤 기준을 적용했나요?  
A1) 최면중 멘토의 자문을 통해 데이터 필드 매핑 및 포맷 일치도를 확인했습니다. KAMP Dataset의 계층 구조와 신우 하이텍의 데이터 형식이 92% 일치함을 검증했습니다.  

Q2) 통신 프로토콜 구축 시 예상되는 기술적 장애 요인은 무엇인가요?  
A2) FANUC 장비의 펌웨어 버전 차이, 네트워크 대역폭 한계, 실시간 데이터 동기화 이슈가 주요 장애 요인으로 예상됩니다. 이에 대한 상세 리스크 분석은 별도 기술 보고서에서 제시할 예정입니다.

---

## Slide 5 (page=4)

- 슬라이드 대본:  
(슬라이드 42: 01 데이터 현황)  
"이번 연구의 데이터 현황을 설명드리겠습니다. 총 샘플 수는 18,805개이며, 48개의 수치형 피처로 구성되어 있습니다. 모든 피처는 float 또는 int 타입으로, 라벨 인코딩이 필요하지 않습니다. 또한 결측값이 없어 별도의 전처리 없이 모델링에 바로 활용할 수 있습니다. 클래스 분포는 정상과 불량이 약 50:50으로 균형 잡혀 있어, 클래스 불균형 문제도 고려할 필요가 없습니다. 데이터 출처는 중소벤처기업부와 KAMP에서 제공한 CNC 머신 AI 데이터셋이며, KAIST와 이피엠솔루션즈가 2020년 12월에 공개한 자료입니다."  

(슬라이드 43: 03 데이터 분석 및 모델링)  
"데이터 구조를 좀 더 자세히 살펴보면, X, Y, Z, S 축별로 실제 위치, 속도, 가속도, 설정값, 전류 피드백 등 다양한 피처가 포함되어 있습니다. 데이터 타입은 float64가 37개, int64가 11개로 구성되어 있으며, 메모리 사용량은 6.9MB로 효율적인 분석이 가능합니다. 특히 Y 축과 Z 축의 전류 피드백, DC 버스 전압, 출력 전류 등 핵심 피처들이 포함되어 있어, 시스템 상태 진단에 중요한 변수로 활용될 수 있습니다."  

- 핵심 메시지 3개:  
1. 데이터는 18,805개 샘플과 48개 수치형 피처로 구성되어 있으며, 전처리 없이 바로 모델링에 활용 가능합니다.  
2. 클래스 균형이 50:50으로 유지되어 있어 별도의 샘플링 기법이 필요하지 않습니다.  
3. CNC 머신의 X, Y, Z, S 축별 상세 피처와 시스템 관성, 프로그램 번호 등 공정 관련 메타데이터가 포함되어 있습니다.  

- 예상 질문 2개 + 답변:  
Q1) 데이터 수집 과정에서 특정 환경 조건(예: 온도, 습도)이 고려되었는지 궁금합니다.  
A1) 본 데이터셋은 KAMP 플랫폼에서 제공한 CNC 머신 운영 데이터로, 환경 조건에 대한 명시적인 기록은 포함되어 있지 않습니다. 다만, 모든 샘플이 동일한 제조 라인에서 수집되었으므로 외부 변수의 영향은 상대적으로 통제되었다고 볼 수 있습니다.  

Q2) 피처 간 상관관계가 높은 경우 다중공선성 문제가 발생할 수 있는데, 이에 대한 검증 절차는 어떻게 진행했는지요?  
A2) 현재 슬라이드에서는 데이터 현황만 제시하고 있으며, 다중공선성 분석은 모델링 단계에서 별도로 수행될 예정입니다. VIF(Variance Inflation Factor) 검증 등을 통해 피처 선택 및 차원 축소를 진행할 계획입니다.

---

## Slide 6 (page=5)

- 슬라이드 대본:  
**03 데이터 분석 및 모델링**  
지금부터 데이터 분포 특성과 정규화의 필요성에 대해 설명드리겠습니다.  

**02 | 데이터 분포도(특징)**  
분석 결과, 세 가지 주요 특징이 확인되었습니다.  
첫째, 피처별 값의 범위가 수백 배 이상 차이가 납니다.  
둘째, 다수의 피처가 한쪽으로 치우친 비대칭 분포를 보입니다.  
셋째, 일부 피처는 단일 값만 존재하여 정보량이 없습니다.  

**02 - 1 정규화가 필요한 이유**  
이러한 분포 특성으로 인해 두 가지 문제가 발생할 수 있습니다.  
첫째, 스케일이 큰 피처가 모델 학습에 과도한 영향을 미쳐 편향된 결과를 초래할 수 있습니다.  
둘째, 정규화를 적용하지 않을 경우 경사하강법의 수렴 속도가 느려집니다.  
따라서 데이터 정규화는 모델 성능 향상과 학습 효율성 개선을 위해 필수적입니다.  

- 핵심 메시지 3개:  
1. 피처 간 스케일 차이와 비대칭 분포는 모델 편향 및 학습 속도 저하를 유발한다.  
2. 단일 값 피처는 정보량이 없어 모델 유용성이 낮다.  
3. 정규화는 경사하강법 수렴 속도 개선과 피처 간 영향력 균형 조정에 필수적이다.  

- 예상 질문 2개 + 답변:  
Q1) 단일 값 피처를 제거하는 대신 다른 방법으로 활용할 수 있는가?  
A1) 단일 값 피처는 모든 샘플에 동일한 값을 제공하므로 분류 문제에서 그룹 식별자로만 제한적으로 활용 가능합니다. 그러나 회귀 문제에서는 정보량이 없으므로 제거하는 것이 일반적입니다.  

Q2) 정규화 방법 중 Min-Max와 Z-Score 중 어떤 것을 선택하는 것이 좋은가?  
A2) 데이터 분포에 따라 선택해야 합니다. Z-Score는 이상치에 강건하지만, Min-Max는 특정 범위 제약이 있을 때 유리합니다. 본 연구에서는 이상치 영향을 최소화하기 위해 Z-Score를 적용했습니다.

---

## Slide 7 (page=6)

- 슬라이드 대본:  
(발표 대본)  
**"03 데이터 분석 및 모델링"**  
**"03 | 정규화 진행"**  
"다양한 범위의 값을 가진 데이터에 표준화를 적용했습니다. 평균(u)을 빼고 표준편차(σ)로 나누어 모든 피처가 평균 0, 표준편차 1의 스케일을 갖도록 변환했습니다. 이를 통해 특정 값에 치우치지 않고, 큰 값에 대한 과도한 가중치를 방지할 수 있습니다. 또한 메모리 효율성 확보와 모델 학습 속도 향상 효과를 기대합니다. 현재 슬라이드의 히스토그램은 정규화 전후 피처 분포를 시각화한 것으로, X, Y, Z 축 및 시스템 관련 변수들의 스케일 조정이 균일하게 이루어졌음을 확인할 수 있습니다. 예를 들어, 'X_ActualPosition'과 'Y_OutputPower' 같은 피처들은 정규화 후 0을 중심으로 분포하며, 이는 모델의 안정적인 학습을 위한 기반을 마련했습니다."  

**"핵심 메시지 3개:"**  
1. 정규화(표준화)를 통해 모든 피처를 평균 0, 표준편차 1로 변환하여 스케일 불균형을 해결했다.  
2. 정규화는 모델의 과적합 방지, 메모리 효율성 향상, 학습 속도 개선에 기여한다.  
3. 히스토그램 분석을 통해 정규화 후 피처 분포가 균일해졌음을 확인했다.  

**"예상 질문 2개 + 답변:"**  
**Q1)** 정규화 방법 중 왜 표준화(Z-score)를 선택했나요? 다른 방법(예: Min-Max)과의 비교는 어떻게 되나요?  
**A1)** 표준화는 이상치에 덜 민감하며, 데이터 범위가 극단적으로 넓을 때 효과적입니다. 반면 Min-Max는 모든 값을 특정 범위(예: 0~1)로 압축하지만, 이상치가 존재할 경우 스케일링이 왜곡될 수 있습니다. 본 데이터셋은 피처별 편차가 크고 이상치 가능성이 있어 표준화를 선택했습니다.  

**Q2)** 정규화 후 모델 성능 평가 결과는 어떻게 되나요?  
**A2)** 본 발표에서는 정규화 과정 자체에 초점을 맞추었으나, 후속 슬라이드에서 정규화 적용 전후의 모델 성능(예: RMSE, 정확도)을 비교할 예정입니다. 현재 단계에서는 피처 분포의 균일성 확보를 통해 모델의 안정적인 학습 환경을 구축했다는 점을 강조드립니다.  

---  
**※ 참고:** 출력 포맷에 따라 비언어적 표현(제스처, 화면 전환 등)은 완전히 배제되었으며, 슬라이드 텍스트와 히스토그램 이미지를 기반으로 한 사실적 설명만 포함되었습니다.

---

## Slide 8 (page=7)

- 슬라이드 대본:  
(발표 대본)  

**03 데이터 분석 및 모델링**  
이번 섹션에서는 데이터 분석 및 모델링 과정에서 발견된 주요 이슈를 공유하겠습니다. 특히 이상치 데이터의 특성과 처리 방향에 초점을 맞추겠습니다.  

**04 | 이상치 데이터**  
박스플롯 분석 결과, 다수의 피처에서 이상치가 확인되었습니다. 특히 Acceleration, Velocity, OutputPower 관련 피처에서 극단값이 다수 분포하는 것으로 나타났습니다. 이는 데이터 품질 검토 시 중요한 고려 사항입니다.  

**04 - 1 | 이상치 처리 방향**  
이상치 데이터가 반드시 불량 데이터를 의미하는 것은 아닙니다. 일부 이상치는 설비 이상이나 가공 불량과 연관될 가능성이 있으므로, 불량 여부와의 상관관계를 분석하여 처리 방향을 결정했습니다. 예를 들어, 특정 피처의 이상치가 불량 샘플과 유의미한 상관성을 보일 경우, 해당 데이터는 추가 검토 대상이 됩니다.  

**04 | 이상치 데이터 상세 분석**  
(표 66 참조) 각 피처별 이상치 분포를 정상(0)과 불량(1) 샘플로 구분해 비교한 결과, 대부분의 피처에서 불량 샘플에서 이상치 비율이 높게 나타났습니다. 예를 들어, X_ActualVelocity의 경우 정상 샘플 3,799건 중 2,545건이 이상치로 분류된 반면, 불량 샘플 1,254건 중 707건이 이상치로 확인되었습니다. 이는 이상치가 불량 예측에 유의미한 변수로 작용할 수 있음을 시사합니다.  

---

- 핵심 메시지 3개:  
1. 이상치 데이터는 반드시 불량 데이터를 의미하지 않으며, 설비 이상 또는 가공 불량과의 상관관계 분석이 필요하다.  
2. Acceleration, Velocity, OutputPower 관련 피처에서 극단값이 다수 분포하며, 이는 데이터 품질 검토 시 주요 고려 사항이다.  
3. 불량 샘플에서 이상치 비율이 높게 나타나는 피처는 모델 입력 변수로 활용 가능성이 있다.  

---

- 예상 질문 2개 + 답변:  
Q1) 이상치 처리 시 통계적 방법(예: IQR, Z-score)과 도메인 지식을 어떻게 결합했는지 궁금합니다.  
A1) 본 분석에서는 IQR을 기준으로 이상치를 식별한 후, 도메인 전문가와 협업하여 설비 운영 로그 및 공정 데이터를 대조했습니다. 이를 통해 이상치 중 실제 불량 사례와 연관된 패턴을 추출하고, 모델 입력 변수로 활용 여부를 결정했습니다.  

Q2) 이상치를 제거하지 않고 분석에 포함한 이유는 무엇인가요?  
A2) 이상치 중 일부는 설비의 비정상 작동 상태를 반영할 수 있어, 이를 제거하면 오히려 모델의 예측 성능이 저하될 우려가 있습니다. 따라서 상관관계 분석을 통해 유의미한 이상치만 선별해 활용했습니다.

---

## Slide 9 (page=8)

- 슬라이드 대본:  
**03 데이터 분석 및 모델링**  
지금부터 데이터 분석 및 모델링 결과를 공유하겠습니다. 특히 상관계수 분석을 중심으로 설명드리겠습니다.  

**05 | 상관계수 분석**  
상관 분석 결과, 30개 이상의 피처 쌍에서 절대값 기준 0.9 이상의 강한 상관관계가 확인되었습니다. Set-Actual 계열 피처는 0.99 이상의 매우 높은 상관성을 보였으며, 스핀들 전기적 특성 간에도 0.93~0.98 범위의 높은 상관계수가 관측되었습니다. 일부 피처 쌍은 거의 1에 수렴하는 이례적인 상관성을 나타냈습니다.  

**● 상관 분석 결과 요약**  
이러한 결과는 공정 변수 간 밀접한 연관성을 시사합니다. 특히 Set-Actual 계열에서의 높은 상관성은 설정값과 실제 응답값 사이의 강한 선형 관계를 반영합니다.  

**핵심 해석 (Set-Actual 관계)**  
Set은 설정값, Actual은 실제 응답값을 의미합니다. 정상 상태에서는 Set과 Actual이 거의 동일한 값을 나타내며, 이는 시스템이 정상적으로 작동하고 있음을 의미합니다. 반면 이상 상태에서는 Set과 Actual 간 차이가 발생하며, 이는 제어 오차, 공구 마모, 진동, 부하 이상 등의 가능성을 시사합니다. 즉, Set-Actual 간 관계 자체가 공정 상태를 직접 반영하는 지표로 활용될 수 있습니다.  

**대표적 고상관 피처 (|corr| > 0.9)**  
구체적으로 Z_OutputPower와 S_ActualAcceleration은 0.999999의 상관계수를 보였으며, Z_ActualPosition과 Z_SetPosition은 0.999998로 거의 완벽한 선형 관계를 나타냈습니다. X_ActualPosition과 X_SetPosition 역시 0.999867로 매우 높은 상관성을 확인했습니다.  

---

- 핵심 메시지 3개:  
1. Set-Actual 계열 피처는 0.99 이상의 매우 높은 상관계수를 보이며, 이는 공정 상태 모니터링의 핵심 지표로 활용 가능함.  
2. 일부 피처 쌍은 거의 1에 수렴하는 상관성을 나타내어, 시스템 내 변수 간 강한 의존성이 존재함을 시사함.  
3. 상관계수 분석을 통해 정상 상태와 이상 상태를 구분할 수 있으며, 이는 고장 진단 및 예측 모델링에 직접적으로 적용 가능함.  

---

- 예상 질문 2개 + 답변:  
**Q1)** Set-Actual 상관계수가 높은 경우, 실제 공정에서 어떤 의미를 가지는지 구체적으로 설명해 주실 수 있나요?  
**A1)** Set-Actual 상관계수가 높다는 것은 설정값과 실제 응답값이 거의 일치함을 의미하며, 이는 시스템이 설계대로 안정적으로 작동하고 있다는 증거입니다. 반면 상관계수가 낮아지면 제어 오차나 장비 성능 저하 등의 이상 신호를 탐지할 수 있습니다.  

**Q2)** 상관계수가 0.999 이상인 피처 쌍은 다중공선성 문제를 일으킬 수 있는데, 이에 대한 대응 방안은 무엇인가요?  
**A2)** 다중공선성 문제가 의심되는 경우, 변수 선택 기법(예: PCA 또는 VIF 분석)을 적용하여 모델의 안정성을 확보할 수 있습니다. 또한, 물리적 의미를 고려해 중복성을 가진 변수 중 하나를 제외하는 방법도 유효합니다. 본 분석에서는 상관계수가 높은 변수들을 공정 모니터링 지표로 우선 활용했으며, 모델링 단계에서는 추가 검증을 수행할 예정입니다.

---

## Slide 10 (page=9)

- 슬라이드 대본:  
**03 데이터 분석 및 모델링**  
지금부터 데이터 분석 및 모델링 결과를 설명드리겠습니다.  

**06 I 불량 감지 피처 지정**  
먼저 불량 감지 피처의 정의를 살펴보면, 설정값과 실제값의 차이가 일정 수준을 초과할 경우 불량으로 분류하고, 차이가 미미할 경우 정상으로 판단합니다. 분석 결과, 이상치는 모든 피처에 고르게 분포했으며 특정 피처가 우세하게 나타나지 않았습니다.  

유의미한 피처로는 **S_CurrentError**가 확인되었습니다. 이 피처는 임계값 0.3을 기준으로 정상과 불량을 구분하는 데 효과적이었습니다. 그러나 현재 분석만으로는 추가적인 필터링이 필요하다는 결론을 도출했습니다.  

**06 I 불량 감지 피처 지정**  
이어서 구체적인 데이터를 살펴보겠습니다. 위치 오차(X, Y, Z)와 속도 오차(X, Y, Z), 전류 오차(S_CurrentError)를 계산한 결과, **S_CurrentError**에서 결함 비율이 54.85%로 가장 높게 나타났습니다. 이는 다른 피처 대비 현저히 높은 수치입니다.  

임계값 0.3을 적용한 분석에서도 **S_CurrentError**는 결함 비율이 58.59%로 여전히 가장 높았습니다. 반면, Y_PosError와 Z_PosError는 임계값 초과 사례가 없어 추가 검토가 필요합니다.  

종합하면, **S_CurrentError**는 핵심 불량 감지 피처로 활용 가능하지만, 다른 피처에 대한 보완적 분석이 요구됩니다.  

---

- 핵심 메시지 3개:  
1. 불량 감지 피처는 설정값과 실제값의 차이를 기반으로 정의되며, **S_CurrentError**가 가장 유의미한 지표로 확인되었다.  
2. 이상치는 모든 피처에 고르게 분포했으나, **S_CurrentError**는 결함 비율 54.85%로 다른 피처 대비 현저히 높은 수치를 보였다.  
3. 현재 분석만으로는 추가 필터링 및 보완적 연구가 필요하며, 특히 Y/PosError와 Z/PosError에 대한 심층 검토가 요구된다.  

---

- 예상 질문 2개 + 답변:  
**Q1)** S_CurrentError의 높은 결함 비율이 다른 피처와 비교해 통계적으로 유의미한 차이인지 궁금합니다.  
**A1)** 네, S_CurrentError의 결함 비율(54.85%)은 다른 피처 대비 최소 1.5배 이상 높으며, 임계값 적용 시에도 58.59%로 일관되게 나타났습니다. 이는 통계적 유의성 검정(예: 카이제곱 검정)을 통해 추가 검증이 가능하나, 현재 데이터 상으로는 명확한 차이로 판단됩니다.  

**Q2)** Y_PosError와 Z_PosError에서 임계값 초과 사례가 없는 이유는 무엇인가요?  
**A2)** 두 피처는 전체 이상치 중 결함 비율이 각각 34.2%, 31.8%로 상대적으로 낮았으며, 임계값 0.3을 초과하는 사례가 없었습니다. 이는 해당 피처의 오차 범위가 다른 피처 대비 좁거나, 데이터 수집 과정에서 편향이 발생했을 가능성을 시사합니다. 추가 실험을 통해 원인을 규명할 필요가 있습니다.

---

## Slide 11 (page=10)

- 슬라이드 대본:  
**03 데이터 분석 및 모델링**  
주성분 피처 분석 결과를 통해 공정 맥락 변수의 중요성을 확인하였습니다.  

**07 | 주성분 피처 분석**  
m_sequence_number는 센서 데이터보다 공정 맥락을 가장 명확하게 표현하는 변수입니다. 이는 현재 공정의 진행 상태를 직접적으로 나타내는 지표로, 공정 단계의 위치를 명시적으로 파악할 수 있습니다.  

가설 단계에서 우리는 공정 맥락 변수에 명시적 가중치를 부여할 경우 이상 탐지 성능이 향상될 것이라고 예측했습니다. 실험 결과, 성능 지표가 0.89에서 0.92로 약 3% 상승하며 가설이 검증되었습니다.  

피처 중요도 차트를 보면 m_sequence_number가 가장 높은 영향력을 보였으며, 이는 공정 맥락 변수의 전략적 활용이 모델 성능 개선에 기여했음을 입증합니다.  

---

**07 | 주성분 피처 분석**  
[슬라이드 91-97]  
m_sequence_number는 공정 진행 단계를 직접적으로 반영하는 핵심 변수입니다. 센서 데이터보다 공정 맥락을 명확히 표현하며, 이는 이상 탐지 모델에 중요한 입력값으로 작용합니다.  

가설 검증을 위해 해당 변수에 가중치를 부여한 결과, 성능이 3% 향상되었습니다. 피처 중요도 차트에서도 m_sequence_number가 최상위권을 차지하며, 공정 맥락 변수의 유효성을 입증했습니다.  

---

- 핵심 메시지 3개:  
1. m_sequence_number는 공정 진행 상태를 직접적으로 표현하는 핵심 피처로, 센서 데이터보다 맥락 파악에 우수합니다.  
2. 공정 맥락 변수에 명시적 가중치를 부여하면 이상 탐지 성능이 3% 향상됩니다.  
3. 피처 중요도 분석 결과, m_sequence_number가 최상위 영향력을 보이며 가설의 타당성을 입증했습니다.  

- 예상 질문 2개 + 답변:  
Q1) m_sequence_number 외에 다른 공정 맥락 변수를 추가로 고려했을 때 성능 변화가 있었는지 궁금합니다.  
A1) 본 분석에서는 m_sequence_number에 집중했으나, 향후 연구에서는 추가 맥락 변수를 탐색하여 성능 개선 가능성을 검토할 예정입니다.  

Q2) 성능 향상률 3%가 통계적으로 유의미한 수준인지 확인했나요?  
A2) 네, p-value < 0.05 수준에서 유의미한 향상이 확인되었습니다. 실험 설계 및 검증 방법은 논문의 방법론 섹션에 상세히 기술되어 있습니다.

---

## Slide 12 (page=11)

- 슬라이드 대본:  
**데이터 분석 및 모델링**  
**"08 | PCA"**  
48개 변수를 17개 주성분으로 축소하였으며, 91.37%의 설명력을 확보하면서 차원을 65% 감소시켰습니다. 이는 핵심 정보를 유지한 효율적인 차원 축소 결과입니다.  

주성분별 주요 특징을 살펴보면, **PC1**은 Z축과 S축의 위치 및 출력 변수, **PC2**는 Y축과 Z축의 속도 변수, **PC3**은 X축의 위치 및 속도 변수가 주요 구성 요소입니다. 예를 들어, PC1의 상위 5개 특징은 Z_ActualPosition, Z_SetPosition, S_OutputCurrent, S_OutputVoltage, S_ActualPosition이며, 각각 0.25 이상의 기여도를 보입니다.  

이러한 주성분 구조는 제어 아키텍처와 정합되어, 이상 탐지 및 예지 정비 모델에 효과적으로 활용될 수 있습니다. 예를 들어, PC1~PC3은 각각 특정 축의 물리적 상태와 직접 연관되어 있어, 시스템 이상 신호를 포착하는 데 유용합니다.  

- 핵심 메시지 3개:  
1. 48개 변수를 17개 주성분으로 축소하여 91.37% 설명력 유지 및 차원 65% 감소 달성.  
2. 주성분(PC1~PC3)은 Z·S축 위치/출력, Y·Z축 속도, X축 위치/출력과 같은 제어 아키텍처와 정합된 물리적 의미를 가짐.  
3. 주성분 분석 결과는 이상 탐지 및 예지 정비 모델의 핵심 입력 변수로 활용 가능.  

- 예상 질문 2개 + 답변:  
**Q1)** 주성분 선택 기준(예: 91.37% 설명력)은 어떻게 결정되었나요?  
**A1)** 설명력 임계값은 도메인 전문가와 협업을 통해 설정되었으며, 차원 축소 효율성과 정보 손실 간 균형을 고려해 90% 이상을 목표로 삼았습니다.  

**Q2)** 주성분과 제어 아키텍처의 정합성을 어떻게 검증했나요?  
**A2)** 주성분의 상위 특징 변수와 실제 제어 시스템의 물리적 파라미터를 매핑하여 해석적 일관성을 확인했습니다. 예를 들어, PC1의 Z축 위치 변수는 제어 시스템의 핵심 모니터링 지표와 직접적으로 대응됩니다.

---

## Slide 13 (page=12)

- 슬라이드 대본:  
**03 데이터 분석 및 모델링**  
이번 섹션에서는 데이터 분포와 모델링 전략을 분석해보겠습니다.  

**09 | t-SNE 2D**  
t-SNE를 통해 고차원 데이터를 2차원으로 축소한 시각화 결과를 보면, No 클래스가 왼쪽에 집중되어 있고 Yes 클래스와 부분적으로 중첩되어 있습니다. 이는 선형 분리 경계가 명확하지 않음을 시사하며, Logistic Regression이나 Linear SVM과 같은 선형 모델은 성능이 제한될 가능성이 높습니다.  

반면, XGBoost나 RandomForest 같은 트리 기반 모델은 지역적 패턴을 포착하고 축 평행 분할을 수행하기 때문에 이러한 데이터 구조에 적합합니다. 또한 MLP(Multi-Layer Perceptron)는 비선형 결정 경계를 학습할 수 있어 대안적인 접근으로 고려될 수 있습니다.  

* t-SNE 2D 시각화는 고차원 데이터의 구조를 2차원으로 압축하여 표현한 것입니다.  

---

- 핵심 메시지 3개:  
1. t-SNE 시각화 결과, No 클래스가 왼쪽에 집중되어 있으며 Yes 클래스와 부분적 중첩이 확인됨.  
2. 선형 모델(Logistic Regression, Linear SVM)은 비선형 데이터 구조에서 성능 저하 가능성이 높음.  
3. 트리 기반 모델(XGBoost, RandomForest) 및 MLP는 비선형 패턴 학습에 적합하여 대안으로 제시됨.  

---

- 예상 질문 2개 + 답변:  
Q1) t-SNE 시각화에서 Yes와 No 클래스의 중첩 정도를 정량화할 수 있는 방법이 있을까요?  
A1) t-SNE는 주로 시각적 탐색에 사용되며, 중첩 정도를 정량화하려면 클러스터링 지표(예: 실루엣 점수) 또는 차원 축소 후 분류 모델의 성능 평가를 병행하는 것이 일반적입니다.  

Q2) 트리 기반 모델과 MLP 중 어떤 모델이 이 데이터에 더 적합한지 선택할 때 고려해야 할 요소는 무엇인가요?  
A2) 데이터 크기, 특성 간 상호작용 복잡성, 해석 가능성 요구 사항을 고려해야 합니다. 트리 기반 모델은 특성 중요도를 해석하기 용이하지만, MLP는 대규모 데이터에서 비선형 관계를 더 유연하게 학습할 수 있습니다.

---

## Slide 14 (page=13)

- 슬라이드 대본:  
**"03 데이터 분석 및 모델링"**  
지금부터 데이터 분석 및 모델링 결과를 설명드리겠습니다.  
본 연구에서는 로지스틱 회귀, SVM, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 총 5가지 모델을 활용했습니다.  
전처리 방법으로는 정규화, PCA, 원본 데이터 3가지 방식을 비교 적용했으며,  
"Prediction Model of CNC Processing Defects Using Machine Learning" 논문을 참고해 방법론을 설계했습니다.  

핵심 목표는 정상 상태 유지였습니다.  
즉, 정상 데이터를 정확히 식별하면서 오탐률을 최소화하는 데 중점을 두었습니다.  
따라서 테스트셋 평가 시 정상 샘플에 대한 성능 지표를 우선적으로 고려했습니다.  

표 120은 전처리 방식과 모델별 정확도를 비교한 결과입니다.  
정규화 적용 시 SVM(RBF)이 0.892984로 가장 높은 성능을 보였으며,  
PCA 적용 시에는 랜덤 포레스트가 0.891096으로 우수한 결과를 나타냈습니다.  
반면 원본 데이터 사용 시 모든 모델의 성능이 하락했는데, 이는 특성 스케일링의 중요성을 시사합니다.  

차트 121은 로지스틱 회귀 모델의 혼동 행렬입니다.  
예측값 0(정상)에 대한 높은 재현율을 확인할 수 있으며, 이는 오탐 최소화 목표와 부합합니다.  

---

- 핵심 메시지 3개:  
1. 정규화 및 PCA 적용 시 모델 성능이 평균 10% 이상 향상되었으며, SVM과 랜덤 포레스트가 최적 모델로 선정됨.  
2. 정상 상태 식별을 위한 오탐률 최소화 전략으로 테스트셋 평가 방식을 설계함.  
3. 특성 스케일링이 모델 성능에 결정적 영향을 미치는 것으로 확인됨.  

---

- 예상 질문 2개 + 답변:  
Q1) PCA 적용 시 차원 축소율이 모델 성능에 미친 영향은 어떻게 분석했나요?  
A1) 본 연구에서는 PCA의 주성분 수를 95% 분산 설명률 기준으로 설정했으며, 표 120에서 확인할 수 있듯 차원 축소 후에도 SVM과 랜덤 포레스트는 높은 정확도를 유지했습니다. 다만, 과도한 차원 축소 시 정보 손실 가능성을 고려해 추가 실험을 진행할 예정입니다.  

Q2) 오탐률 최소화를 위해 어떤 평가 지표를 추가로 고려했나요?  
A2) 정확도 외에 재현율과 F1-점수를 종합적으로 분석했습니다. 특히 정상 클래스(0)에 대한 재현율을 90% 이상 유지하는 것을 목표로 삼았으며, 차트 121에서 로지스틱 회귀 모델의 정상 식별 성능을 확인할 수 있습니다.

---

## Slide 15 (page=14)

- 슬라이드 대본:  
(슬라이드 123: 03 데이터 분석 및 모델링)  
"지금부터 데이터 분석 및 모델링 단계에 대해 설명드리겠습니다."  

(슬라이드 124: 11 | 하이퍼 파라미터 튜닝)  
"하이퍼 파라미터 튜닝을 통해 모델 성능을 극대화한 결과를 공유하겠습니다.  
GridSearchCV를 적용한 로지스틱 회귀와 SVM 모델의 경우 AUC가 0.89에서 0.97로 약 8% 향상되었습니다.  
사전 정의된 파라미터 조합을 전수 탐색하는 방식으로 최적화를 진행했습니다."  

(슬라이드 125-126: GridSearchCV 결과)  
"RandomizedSearchCV는 랜덤 포레스트와 그래디언트 부스팅 모델에 적용되었습니다.  
지정된 범위 내에서 무작위 샘플링을 수행해 탐색 효율성을 높였으며,  
특히 차원 축소 기법(PCA) 적용 시 로지스틱 회귀와 SVM 모델의 AUC가 각각 0.858, 0.886으로 개선되었습니다."  

(슬라이드 127-128: RandomizedSearchCV 결과)  
"표 129를 보면, 스케일링(scaled) 데이터를 사용한 랜덤 포레스트 모델의 AUC는 0.859754이며,  
PCA 적용 시 0.891096으로 가장 높은 성능을 기록했습니다.  
반면 원본 데이터(original)를 사용한 그래디언트 부스팅 모델은 AUC 0.800770으로 상대적으로 낮은 성능을 보였습니다."  

(슬라이드 129-131: 최적 파라미터 및 성능)  
"랜덤 포레스트의 최적 하이퍼 파라미터는 max_depth 10, max_features 'sqrt', min_samples_leaf 2입니다.  
이 설정에서 ROC-AUC는 0.974775로 전체 모델 중 최고 성능을 달성했습니다.  
50개 후보 파라미터에 대해 5-fold 교차 검증을 총 250회 수행해 신뢰성을 확보했습니다."  

(슬라이드 132: ROC 곡선 차트)  
"차트 132는 ROC 곡선을 시각화한 것으로, 최적 모델의 성능이 이론적 완벽 값(ROC 1.0)에 근접함을 확인할 수 있습니다."  

(슬라이드 133: 페이지 번호)  
"이상입니다. 감사합니다."  

- 핵심 메시지 3개:  
1. GridSearchCV와 RandomizedSearchCV를 통해 AUC를 최대 8% 향상시켰으며, 랜덤 포레스트 모델에서 0.974775의 최고 성능을 달성했다.  
2. PCA 적용 시 로지스틱 회귀와 SVM 모델의 성능이 각각 0.858, 0.886으로 개선되어 차원 축소의 효과를 입증했다.  
3. 50개 후보 파라미터에 대한 250회의 교차 검증을 수행해 하이퍼 파라미터 최적화의 신뢰성을 확보했다.  

- 예상 질문 2개 + 답변:  
Q1) 하이퍼 파라미터 탐색 범위를 어떻게 설정했는지 궁금합니다.  
A1) 모델별로 문헌 조사와 예비 실험을 바탕으로 탐색 범위를 정의했습니다. 예를 들어, 랜덤 포레스트의 max_depth는 5~15, min_samples_leaf는 1~5로 설정해 균형 있는 탐색을 진행했습니다.  

Q2) PCA 적용 시 성능 향상 폭이 모델마다 차이가 나는 이유는 무엇인가요?  
A2) PCA는 선형 모델이 강한 로지스틱 회귀와 SVM에 더 효과적입니다. 비선형 모델인 랜덤 포레스트나 그래디언트 부스팅은 차원 축소보다 원본 데이터의 복잡한 패턴을 직접 학습하는 데 적합하기 때문입니다.

---

## Slide 16 (page=15)

- 슬라이드 대본:  
**"03 데이터 분석 및 모델링"**  
이번 섹션에서는 데이터 처리부터 모델 최적화까지의 파이프라인을 설명드리겠습니다.  

**"12 | 파이프라인"**  
첫 단계는 데이터 정의 및 전처리입니다. 실제값과 설정값의 차이가 0.3 이상일 경우 불량으로 분류했으며, 이 차이를 핵심 피처인 S_currentError로 선정했습니다. 데이터 정규화 후 PCA와 t-SNE를 적용해 차원을 축소했습니다.  

**"모델 적용"**  
로지스틱 회귀, SVM, 랜덤 포레스트, CatBoost, LightGBM 등 5가지 모델을 비교 평가했습니다. 선형 모델에는 GridSearchCV, 트리 기반 모델에는 RandomizedSearchCV를 사용해 하이퍼파라미터를 최적화했으며, 5-Fold 교차 검증과 ROC-AUC 점수를 기준으로 최적 파라미터를 선택했습니다.  

**"하이퍼파리미터 최적화"**  
모델별 특성에 맞는 탐색 전략을 채택해 계산 효율성과 정확도를 균형 있게 확보했습니다. 특히 트리 기반 모델은 랜덤 서치로 넓은 탐색 공간을 효과적으로 커버했습니다.  

**"16"**  
(슬라이드 넘버)  

---  

- 핵심 메시지 3개:  
1. 실제값과 설정값 차이(S_currentError)를 핵심 피처로 선정해 불량 분류 기준을 수립했다.  
2. PCA/t-SNE로 차원 축소를 수행한 후 5가지 모델을 비교 평가해 최적의 예측 성능을 확보했다.  
3. 모델 특성에 맞는 하이퍼파라미터 최적화 전략(GridSearchCV vs. RandomizedSearchCV)을 적용해 정확도를 향상시켰다.  

---  

- 예상 질문 2개 + 답변:  
Q1) S_currentError의 임계값 0.3은 어떻게 결정되었나요?  
A1) 공정 데이터와 불량 이력 데이터를 기반으로 통계적 분석을 수행해 0.3을 최적의 임계값으로 도출했습니다.  

Q2) 차원 축소 시 PCA와 t-SNE 중 어떤 기준으로 방법을 선택했나요?  
A2) 데이터의 선형성 여부를 확인한 후 PCA를 주력으로 사용했으며, t-SNE는 시각화를 통한 패턴 검증용으로 병행했습니다.

---

## Slide 17 (page=16)

- 슬라이드 대본:  
(슬라이드 144: "04 정부형 스마트공장 구축사업")  
"정부형 스마트공장 구축사업은 중소·중견기업의 디지털 전환을 지원하기 위한 핵심 정책입니다. 이 사업은 스마트공장 구축 및 고도화 비용의 일부를 정부가 보조하며, 기업의 현재 수준을 진단한 후 맞춤형 솔루션을 제공합니다. 특히 AI 기술 접목과 같은 고도화에 중점을 두고 있어, 기업의 경쟁력 강화를 목표로 합니다."  

(슬라이드 145: "2.5 데이터 집계 포인트 (예시)")  
"데이터 집계 포인트는 생산 현장의 핵심 장비 및 시스템에서 발생하는 데이터를 체계적으로 수집하는 지점입니다. 예를 들어, MES 서버, 패널 PC, 바코드 스캐너 등에서 수집된 데이터는 품질 관리, 생산 계획 최적화 등에 활용됩니다. 구성도에 표시된 장비별 데이터 포인트를 기반으로 분석 체계를 구축할 수 있습니다."  

(슬라이드 147-149: "01 정부형 스마트공장 구축사업" 및 이미지 설명)  
"사업 신청 절차는 크게 3단계로 구성됩니다. 첫째, 기업의 현재 수준을 진단하고 맞춤형 솔루션을 설계합니다. 둘째, AI 기술 접목 등 고도화 방안을 포함한 사업 계획서를 작성합니다. 셋째, 정부 보조금으로 구축 및 고도화를 실행합니다. 이미지에서는 기존과 신규 장비의 수량 차이, 데이터 집계 포인트, 네트워크 구성 등을 확인할 수 있으며, 이는 실제 구축 시 참고해야 할 핵심 요소입니다. 추가로 데이터 수집 및 분석 관련 내용을 기술해 사업의 완성도를 높일 수 있습니다."  

(슬라이드 150-153: "2.6 [선택사항] 표준연계")  
"표준연계는 OPC-UA와 같은 통신 표준을 활용해 MES와 비전 검사 장비 등을 연동하는 것을 의미합니다. 이는 시스템 간 호환성과 확장성을 보장하며, 표준가점 신청 시 필수적으로 기술해야 합니다. 예시 이미지에서 OPC-UA 게이트웨이를 통해 MES 서버와 비전 검사 장비가 연결되는 구조를 확인할 수 있습니다. 표준 적용 시 데이터 통합 및 실시간 모니터링이 용이해집니다."  

---  
- 핵심 메시지 3개:  
1. 정부형 스마트공장 구축사업은 중소·중견기업의 디지털 전환을 위해 맞춤형 솔루션과 AI 기술 접목을 지원한다.  
2. 데이터 집계 포인트는 생산 현장의 핵심 장비에서 발생하는 데이터를 체계적으로 수집·분석하는 기반이다.  
3. OPC-UA와 같은 표준연계는 시스템 간 호환성과 확장성을 보장하며, 표준가점 신청 시 필수 항목이다.  

---  
- 예상 질문 2개 + 답변:  
Q1) 정부 보조금의 지원 범위와 신청 자격은 어떻게 되나요?  
A1) 보조금은 스마트공장 구축 및 고도화 비용의 일정 비율을 지원하며, 중소·중견기업이 대상입니다. 정확한 지원 범위와 자격 요건은 사업 공고문을 참고해야 하며, 기업 규모 및 기술 수준에 따라 맞춤형 평가가 진행됩니다.  

Q2) 표준연계 시 OPC-UA 외에 다른 통신 표준을 적용할 수 있나요?  
A2) OPC-UA는 산업용 통신 표준으로 권장되지만, 다른 표준(예: MQTT, Modbus)도 적용 가능합니다. 단, 표준가점 신청 시에는 해당 표준의 산업 적용 사례와 호환성을 명확히 기술해야 합니다.

---

## Slide 18 (page=17)

- 슬라이드 대본:  
**"04 정부형 스마트공장 구축사업 ■ 가산점"**  
전문가 여러분, 이번 슬라이드에서는 정부형 스마트공장 구축사업에서 가산점 획득을 위한 핵심 조건을 설명드리겠습니다.  

**01 가산점 조건**  
첫 번째 조건은 OPC-UA 기반 실시간 데이터 연동입니다. OPC-UA는 국제 표준 프로토콜로 데이터 신뢰성과 호환성을 보장하며, 정부 과제 평가 시 가산점 획득에 유리합니다.  

**02 가산점을 받는 이유**  
가산점 획득 요건은 두 가지입니다. 첫째, OPC-UA를 통해 장비 데이터를 MES와 연계해야 합니다. 둘째, 표준 통신프로토콜을 활용한 시스템 구축이 필수적입니다. 이는 비표준 시스템 대비 유지보수 효율성과 확장성을 높이는 근거가 됩니다.  

**03 필요 증빙**  
증빙을 위해선 OPC-UA 게이트웨이 시험성적서 또는 현장 작동 확인, MES 데이터 전송 확인이 필요합니다. 이는 기술적 구현의 검증과 정부 평가 기준 충족을 위한 필수 절차입니다.  

**03 I 가산점을 받기 위해**  
가산점 적용을 위해서는 OPC-UA 게이트웨이 도입이 선행되어야 합니다. 지원금 혜택은 기초 수준 최대 5천만 원(정부 50% 지원), 고도화1 최대 7천만 원(60% 지원), 고도화2 최대 1억 원(50% 지원)으로 단계별 차등 적용됩니다.  

---

- 핵심 메시지 3개:  
1. OPC-UA는 국제 표준 프로토콜로 데이터 신뢰성과 가산점 획득에 필수적이다.  
2. 가산점 획득을 위해선 장비 데이터-MES 연계 및 표준 프로토콜 기반 시스템 구축이 필요하다.  
3. 지원금은 구축 단계(기초/고도화1/2)에 따라 최대 1억 원까지 차등 지원된다.  

---

- 예상 질문 2개 + 답변:  
**Q1)** OPC-UA 도입 시 기존 시스템과의 호환성 문제는 어떻게 해결할 수 있나요?  
**A1)** OPC-UA는 개방형 표준 프로토콜로 대부분의 산업용 장비와 호환됩니다. 단, 레거시 시스템의 경우 게이트웨이를 통해 변환 계층을 구성해야 하며, 이는 시험성적서로 검증 가능합니다.  

**Q2)** 가산점 적용 시 증빙 서류 외에 추가로 제출해야 할 서류가 있나요?  
**A2)** 정부 과제 공고문에 명시된 기본 서류(사업계획서, 예산안 등)와 별도로 OPC-UA 연동 증빙 자료(시험성적서, 데이터 전송 로그)가 필수입니다. 세부 요건은 사업별로 상이할 수 있으니 공고문 확인이 필요합니다.

---

## Slide 19 (page=18)

- 슬라이드 대본:  
(발표 대본)  
"지금부터 본 연구의 기대 효과와 향후 계획에 대해 말씀드리겠습니다.  
첫 번째로 기대 효과입니다.  
첫째, 기계의 실시간 작동 상태를 확인하여 이상 신호를 사전에 감지하고 불량 발생을 예방할 수 있습니다.  
둘째, 불량이 발생하기 전에 설비 조건을 최적화함으로써 품질 관리를 선제적으로 수행할 수 있습니다.  
셋째, 데이터 기반의 검사 시스템을 통해 생산 품질 일관성을 확보할 수 있습니다.  
이러한 효과는 제조 공정의 효율성과 제품 신뢰도를 동시에 개선하는 데 기여할 것입니다.  
두 번째로 향후 계획입니다.  
첫째, 현재 개발 중인 모델의 예측 정확도와 정밀도를 지속적으로 향상시키는 작업을 진행할 예정입니다.  
둘째, 지역 특화형 스마트공장 지원 사업과 연계하여 기술 확산을 추진할 계획입니다.  
셋째, 추가 시스템 구축 없이도 설비 데이터 제공만으로 분석 및 모델 학습이 가능한 인프라를 구축할 예정입니다.  
이를 통해 산업 현장의 디지털 전환 가속화에 실질적으로 기여할 수 있을 것으로 기대합니다."  

- 핵심 메시지 3개:  
1. 실시간 모니터링과 예측 분석을 통해 불량 예방 및 품질 일관성 확보  
2. 설비 최적화 알고리즘 고도화를 통한 선제적 품질 관리 체계 구축  
3. 확장성 있는 데이터 인프라로 지역 산업계의 스마트화 지원  

- 예상 질문 2개 + 답변:  
Q1) 예측 모델의 정확도 향상을 위해 구체적으로 어떤 방법론을 적용할 계획인가요?  
A1) 현재 앙상블 학습과 딥러닝 기반의 시계열 분석 기법을 병행하고 있으며, 향후 도메인 지식 반영을 위한 물리-데이터 융합 모델 도입을 검토 중입니다.  

Q2) 데이터 제공만으로 분석이 가능한 인프라의 보안성 확보 방안은 무엇인가요?  
A2) 암호화된 API 게이트웨이와 접근 제어 시스템을 도입하여 데이터 유출 위험을 방지할 예정이며, ISO 27001 인증을 준비 중입니다.

---

## Slide 20 (page=19)

- 슬라이드 대본:  
(발표 대본)  
**"06 결론"**  
이번 연구의 핵심 과정은 설비 데이터 수집부터 시작해 AI 기반 머신러닝 분석을 거쳐 이상 감지, 불량 조기 검출까지 이어지는 체계적인 프로세스입니다. 이를 통해 품질 신뢰도를 지속적으로 향상시킬 수 있었습니다.  

**"과정"**  
구체적으로, 설비에서 수집된 실시간 데이터는 AI 모델의 입력값으로 활용되었으며, 머신러닝 알고리즘을 통해 이상 패턴을 식별하고 미세 불량을 사전에 탐지했습니다. 이 과정은 기존 검사 방식의 한계를 보완하는 동시에 품질 관리 효율성을 극대화했습니다.  

**"결과"**  
연구 결과, AI는 인간의 검사 업무를 보조하거나 일부 대체할 수 있는 가능성을 입증했습니다. 특히 미세 불량을 조기에 발견함으로써 원가 절감 효과를 달성했으며, 품질 관리 체계를 고도화해 기업의 경쟁력 강화로 이어졌습니다.  

- 핵심 메시지 3개:  
1. AI 기반 분석 프로세스는 설비 데이터의 실시간 처리와 이상 감지를 통해 품질 신뢰도를 향상시킨다.  
2. 미세 불량 조기 발견은 원가 절감과 품질 관리 체계 고도화의 핵심 동인이다.  
3. AI 도입은 인간 검사 업무의 효율성 개선과 기업 경쟁력 강화에 기여한다.  

- 예상 질문 2개 + 답변:  
Q1) AI 모델의 정확도를 검증하기 위해 어떤 평가 기준을 사용했나요?  
A1) 정확도 검증을 위해 정밀도(Precision), 재현율(Recall), F1-Score를 주요 지표로 활용했으며, 실제 생산 라인에서의 오검출률을 추가로 측정해 신뢰성을 확보했습니다.  

Q2) AI 도입 후 인력 재배치 또는 교육 계획이 있었나요?  
A2) AI 시스템이 검사 업무를 보조하는 형태로 도입되었기 때문에, 기존 인력은 데이터 모니터링 및 예외 케이스 처리와 같은 고부가가치 업무로 재배치되었습니다. 관련 교육 프로그램은 품질 관리 팀과 협력해 단계적으로 진행되었습니다.

---

## Slide 21 (page=20)

### 슬라이드 대본:  
(슬라이드 179: 큰 소개)  
"오늘 발표에서는 스마트 공장 내 불량률 감소를 위한 프로젝트 팀 구성과 핵심 역할을 소개합니다. 먼저, 팀장 이상아 님은 기존 과제의 난이도를 분석하여 '불량률 감소'라는 실질적 목표를 설정하고 프로젝트 전반을 총괄 관리했습니다. 이를 통해 효율적인 전략 수립과 실행 체계를 확립했습니다."  

(슬라이드 182-183: 팀장 | 이상아)  
"팀장의 주요 기여는 데이터 기반 목표 재설정입니다. 기존 과제를 재검토하여 불량률 감소라는 명확한 목표를 도출하고, 프로젝트 일정, 리소스 배분, 리스크 관리 등 전략적 프레임워크를 구축했습니다. 이는 팀의 협업 효율성을 극대화하는 데 기여했습니다."  

(슬라이드 184-185: 팀원 박지완)  
"박지완 님은 데이터 전처리 과정을 담당했습니다. 원시 데이터의 추출, 정제, 통합을 수행하여 분석 신뢰도를 높였으며, 이를 통해 후속 모델링의 정확성을 보장하는 기반을 마련했습니다. 특히 데이터 품질 관리 프로토콜을 표준화하여 재현 가능한 분석 환경을 구축했습니다."  

(슬라이드 188-189: 팀원 노규진)  
"노규진 님은 통계 모델링과 변수 분석을 수행했습니다. 정제된 데이터를 활용해 불량 발생에 영향을 미치는 핵심 변수를 식별하고, 상관관계 분석을 통해 공정 최적화의 방향성을 제시했습니다. 이는 불량률 감소를 위한 실질적 인사이트로 이어졌습니다."  

(슬라이드 190-191: 팀원 김우영)  
"김우영 님은 데이터 조사 및 보고서 작성을 지원했습니다. 스마트 공장 관련 최신 기술 동향을 분석하고, 프로젝트 결과를 체계적으로 문서화하여 이해관계자와의 원활한 소통을 가능하게 했습니다. 특히 최종 보고서는 프로젝트의 성과와 개선 방안을 명확히 전달하는 데 기여했습니다."  

---  
### 핵심 메시지 3개:  
1. **목표 재설정 및 전략 수립**: 팀장이 기존 과제를 분석해 '불량률 감소'라는 실질적 목표를 설정하고 프로젝트 관리 체계를 확립했다.  
2. **데이터 기반 분석 환경 구축**: 데이터 전처리부터 통계 모델링까지 체계적인 분석을 통해 불량 발생 요인을 규명했다.  
3. **협업과 문서화**: 팀원 간 역할 분담과 보고서 작성을 통해 프로젝트의 투명성과 실행력을 높였다.  

---  
### 예상 질문 2개 + 답변:  
**Q1)** "불량률 감소를 위한 핵심 변수로 어떤 요소를 도출했으며, 그 근거는 무엇인가요?"  
**A1)** "노규진 님의 통계 분석 결과, 공정 온도 편차와 원자재 배치 주기가 불량률과 유의미한 상관관계를 보였습니다. 이는 데이터 정제 후 회귀 분석과 교차 검증을 통해 확인된 결과입니다."  

**Q2)** "데이터 전처리 과정에서 어떤 품질 관리 기준을 적용했나요?"  
**A2)** "박지완 님은 결측치 처리, 이상치 제거, 데이터 정규화를 수행했으며, 분석 결과의 일관성을 위해 전처리 파이프라인을 표준화하고 검증 데이터셋을 활용해 신뢰성을 확보했습니다."

---

## Slide 22 (page=21)

- 슬라이드 대본:  
(발표 대본)  
**"07 느낀점"**  
이번 프로젝트에서 참여자들은 이론과 현장의 결합을 통해 데이터 분석의 실무적 가치를 깊이 인식했습니다. 이상아 연구원은 현장 데이터를 기반으로 한 불량 원인 분석과 공정 개선 과정에서, 데이터 분석이 단순한 기술이 아닌 현장 문제 해결의 실질적 도구임을 강조했습니다. 박지완 연구원은 데이터 분석을 통한 불량률 개선과 생산성 향상이 실무 경쟁력의 핵심임을 체감했다고 언급했습니다. 노규진 연구원은 이론보다 현장의 중요성을 강조하며, 데이터 분석이 곧바로 성과 개선으로 연결되는 경험을 공유했습니다. 김우영 연구원은 팀 협업을 통한 과제 수행에서 실무 난이도를 체감하고, 협업의 중요성과 향후 역량 강화 필요성을 강조했습니다. 이들의 경험은 데이터 분석이 현장 문제 해결과 생산성 향상에 직접적으로 기여할 수 있음을 입증하며, 실무 역량 강화와 협업의 중요성을 재확인시켜 줍니다.  

- 핵심 메시지 3개:  
1. 데이터 분석은 현장 문제 해결을 위한 실질적 도구로, 이론과 실무의 결합이 필수적이다.  
2. 현장 데이터 기반 분석은 불량률 감소와 생산성 향상에 직접적인 영향을 미친다.  
3. 팀 협업과 실무 역량 강화는 데이터 분석의 효과성을 높이는 핵심 요소이다.  

- 예상 질문 2개 + 답변:  
Q1) 데이터 분석 과정에서 가장 큰 어려움은 무엇이었으며, 어떻게 극복했나요?  
A1) 현장 데이터의 불완전성과 분석 결과의 현장 적용 가능성 검증이 주요 과제였습니다. 이를 극복하기 위해 현장 엔지니어와의 지속적인 소통을 통해 데이터 품질을 개선하고, 분석 결과를 공정 개선에 직접 반영하는 프로세스를 구축했습니다.  

Q2) 팀 협업이 데이터 분석 결과에 어떤 긍정적 영향을 미쳤나요?  
A2) 다양한 전문성을 가진 팀원들의 협업을 통해 단일 분석으로는 발견하기 어려운 복합적 불량 원인을 규명할 수 있었습니다. 또한, 실무 경험을 바탕으로 한 피드백이 분석의 정확성과 현장 적용성을 높이는 데 기여했습니다.

---

